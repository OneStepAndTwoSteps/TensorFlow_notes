
## 模型损失函数设计

### 交叉熵（Cross-Entropy, CE）

我们使用交叉熵作为该模型的损失函数。
虽然 Categorical / Binary CE 是更常用的损失函数，不过他们都是 CE 的变体。 CE 定义如下：
![Image_text](https://raw.githubusercontent.com/OneStepAndTwoSteps/TensorFlow_notes/master/static/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/1.png)

对于二分类问题 (C‘=2) ，CE 定义如下:

![Image_text](https://raw.githubusercontent.com/OneStepAndTwoSteps/TensorFlow_notes/master/static/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/2.png)



### Categorical CE Loss（Softmax Loss）

常用于输出为 One-hot 向量的多类别分类（Multi-Class Classification）模型。
![Image_text](https://raw.githubusercontent.com/OneStepAndTwoSteps/TensorFlow_notes/master/static/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/3.png)

Categorical CE Loss（Softmax Loss）就是一个softmax激活函数+交叉熵损失函数

我们之前手写体数字识别的时候使用的交叉熵就是我们Categorical CE Loss，常用语输出一些one-hot向量的多类别的分类模型，softmax激活函数我们之前没写详细介绍，sigmoid函数也是一种激活函数，常用于二分类的问他，softmax对比而言更适合用于多分类(类别大于2)，这里的 f(s)i 就是他输出的多分类的概率。


加入我们的模型是10分类的，我们使用Categorical CE Loss，是非常合适的因为我们输入了手写体数字的图片，刚好我们有10各类别(0-9)经过Categorical CE Loss输出的值其实就是一个向量，这个向量有10个值，向量中的每一个值代表他可能是那个类别的概率，那我们取得最大的值对应的类别，就是我们认为的推理值。

其实我们之前的模型中有四个全连接网络，每个全连接网络都可以使用Softmax Loss去做损失函数，但是由于我们的输出值是一个由4个one-hot向量拼接起来的one-hot向量，这个向量和我们的one-hot向量又有一些不同，因为对于一个one-hot最理想的状态是只有一个值为1，所以说Softmax Loss虽然使用于多分类的类型，但是他只适合单标签的分类模型，什么意思呢？就是输入的图像中只能有一个类别，如果多余一个类别的时候其效果就没有那么好，因为他只能标注出最有可能的物体属于哪个类别，对于我们验证码来说，我们需要识别四个字符，如果使用这个方法的话我们一个使用四个Softmax Loss才对，但是因为我们输出的向量是一个拼接后的向量，所以说不太适合使用Softmax Loss，

这个时候我们其实是比较适用Binary CE Loss   	


### Binary CE Loss（Sigmoid CE Loss）

就是一个Sigmoid激活函数+交叉熵损失函数
与 Softmax Loss 不同，Binary CE Loss 对于每个向量分量（class）都是独立 的，这意味着每个向量分量计算的损失不受其他分量的影响。
因此，它常被用于多标签分类（Multi-label classification）模型。

![Image_text](https://raw.githubusercontent.com/OneStepAndTwoSteps/TensorFlow_notes/master/static/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/4.png)


与 Softmax Loss 不同，Binary CE Loss 对于每个向量分量（class）都是独立 的，什么意思呢？以手写体数字为例他有10个分量，每一个分量都代表一类别0-9，你输出的值对于每一个类别都有一个概率，我们取概率最大的值作为我们的类别，这十个数都是独立的，如果在单纯的手写体识别这10个数字其实是有一定的相关性的。

在我们的验证码识别中其实是一个多分类的模型，比如我们输入了一个图片，图片里有4个数字，每一个数字都是十个类别中的一类，比如说我们输入了1234，1就是十个类别中的一类，他对应这一个one-hot的向量，如果使用Binary CE Loss，它的好处在于我们去计算每一个向量的分量的时候其实他的损失值是不受其他分量的影响的，所以他就变成了一个单独长为40的向量当中每一个值的概率是多少，就比如说我们长度为40的向量，里面有四个1，分别表示不同的字符，如果使用Categorical loss的话，那么他40个元数加起来才等于1，如果使用Binary CE Loss其实就无所谓，因为Binary CE Loss会单独的去计算这40个元素中的分量，单独的去评估最左边的字符属于什么数字，或者说和哪个数最相像。因此，它常被用于多标签分类（Multi-label classification）模型。


可能很多人认为Binary CE Loss只能用在二分类的模型，但是其实每个向量分量计算的损失不受其他分量的影响的特性也常被用于多标签分类（Multi-label classification）模型。每一个元素的值都不会受到影响。








