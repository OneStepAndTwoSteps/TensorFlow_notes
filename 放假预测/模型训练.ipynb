{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 3) <class 'numpy.ndarray'>\n",
      "(47, 1) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 数据归一化操作\n",
    "def normalize_feature(df):\n",
    "    return df.apply(lambda column: (column - column.mean()) / column.std())\n",
    "\n",
    "\n",
    "df = normalize_feature(pd.read_csv('data1.csv',\n",
    "                                   names=['square', 'bedrooms', 'price']))\n",
    "\n",
    "ones = pd.DataFrame({'ones': np.ones(len(df))})# 生成一行ones，其是n行1列的数据框，表示x0恒为1\n",
    "df = pd.concat([ones, df], axis=1)  # 根据列将ones数据和我们从data1文件中读取出的数据合并\n",
    "\n",
    "#将x(输入数据)和y(输出数据)进行分离，用于训练\n",
    "X_data = np.array(df[df.columns[0:3]]) \n",
    "y_data = np.array(df[df.columns[-1]]).reshape(len(df), 1)\n",
    "\n",
    "print(X_data.shape, type(X_data))\n",
    "print(y_data.shape, type(y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建线性回归模型（数据流图）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__get_variable方法和variable方法相比有什么好处呢？__\n",
    "\n",
    "在训练的时候我们使用get_variable方法可以直接获取到我们当前数据流图中名为你设置的名称这个变量的值，以上图为例，就是我们获取名为weight的变量的值。\n",
    "也就是说在我们进行反复训练的时候，我们可能会从我们的checkpoint中获取我们的之前的模型参数，他可以快速的帮我们找到我们模型参数的值，当我们要加载我们的模型参数到数据流图中时，我们就可以通过我们的key:value的格式找到我们对应参数的值了。对我们进行参数的赋值加载有一个很好的作用。 同时也支持我们initializer初始化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-18a7644e846d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hypothesis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# 权重变量 W，形状[3,1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"weights\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m# 假设函数 h(x) = w0*x0+w1*x1+w2*x2, 其中x0恒为1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# 推理值 y_pred  形状[47,1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1477\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1479\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1218\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1220\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mD:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    545\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mD:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    497\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[1;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    846\u001b[0m         \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"tensorflow/python\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" % (err_msg, \"\".join(\n\u001b[1;32m--> 848\u001b[1;33m             traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "alpha = 0.01 # 学习率 alpha\n",
    "epoch = 500 # 训练全量数据集的轮数\n",
    "\n",
    "#首先我们定义我们的输入数据和输出数据的模型(数据流图)，因为我们是在会话中训练数据的时候才把我们的输入数据和输出数据填充进数据流图，\n",
    "#所以我们先用占位符创建一个数据类型和数据形状即可，在经过我们的归一化操作之后他是一个浮点数，所以我们使用浮点数定义，\n",
    "#同时定义我们的X(输入数据的形状为[47,3]，根据我们之前的数据形状定义) y也同理。\n",
    "with tf.name_scope('input'):\n",
    "    # 输入 X，形状[47, 3]\n",
    "    X = tf.placeholder(tf.float32, X_data.shape, name='X')\n",
    "    # 输出 y，形状[47, 1]\n",
    "    y = tf.placeholder(tf.float32, y_data.shape, name='y')\n",
    "\n",
    "#定义我们的权重变量，因为我们的假设函数为个 h(x) = w0*x0+w1*x1+w2*x2，而且从我们的输入数据X中可以看到变量数量是3个，\n",
    "#所以我们W的形状应该是[3,1] ，所以我们代码中(X_data.shape[1], 1)表示的就是我们的行数和X中的列数保持一致也就是3，同时将列数设置为1 ,\n",
    "#W就是我们常用公式hθ(x)=θ^Tx中θ的转置。此时我们在计算我们的假设函数hθ(x)时我们就可以直接进行我们的矩阵相乘 hθ(x)=tf.matmul(X, W)\n",
    "       \n",
    "with tf.name_scope('hypothesis'):\n",
    "    # 权重变量 W，形状[3,1]\n",
    "    W = tf.get_variable(\"weights\",\n",
    "            (X_data.shape[1], 1),initializer=tf.constant_initializer())\n",
    "    # 假设函数 h(x) = w0*x0+w1*x1+w2*x2, 其中x0恒为1\n",
    "    # 推理值 y_pred  形状[47,1]\n",
    "    print(W)\n",
    "    #之前我们有说个hθ(x)=θ^Tx，这里的W等于θ^Tx，所以求假设函数hθ(x)时我们就可以直接进行我们的矩阵相乘 hθ(x)=tf.matmul(X, W)\n",
    "    #就是我们的[47,3]这个形状的矩阵乘以[3,1]这个形状的矩阵。最后我们的到的就是[47,1] 47行1列的预测值\n",
    "\n",
    "    y_pred = tf.matmul(X, W, name='y_pred')\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    # 损失函数采用最小二乘法，y_pred - y 是形如[47, 1]的向量。\n",
    "    # tf.matmul(a,b,transpose_a=True) 表示：矩阵a的转置乘矩阵b，即 [1,47] X [47,1] transpose_a=True 表示转置a矩阵\n",
    "    # 损失函数操作 loss\n",
    "    loss_op = 1 / (2 * len(X_data)) * tf.matmul((y_pred - y), (y_pred - y), transpose_a=True)\n",
    "with tf.name_scope('train'):\n",
    "    # 随机梯度下降优化器 opt\n",
    "    #这里使用梯度下降的方法进行优化，这条代码定义了一个随机梯度优化器，同时我们给优化器传入一个learning_rate=alpha 也就是我们的学习率，这时候我们就定义好了一个优化器opt。\n",
    "    #然后我们通过train_op=opt. minimize(loss_op)，我们通过传入我们的损失函数，通过minimize不断的进行最小化。这个时候我们得到的模型参数就是我们想要的模型参数。\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate=alpha).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建会话（运行环境）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 \t Loss=0.4116 \t Model: y = 0.0791x1 + 0.03948x2 + 3.353e-10\n",
      "Epoch 20 \t Loss=0.353 \t Model: y = 0.1489x1 + 0.07135x2 + -5.588e-11\n",
      "Epoch 30 \t Loss=0.3087 \t Model: y = 0.2107x1 + 0.09676x2 + 3.912e-10\n",
      "Epoch 40 \t Loss=0.2748 \t Model: y = 0.2655x1 + 0.1167x2 + -1.863e-11\n",
      "Epoch 50 \t Loss=0.2489 \t Model: y = 0.3142x1 + 0.1321x2 + 1.77e-10\n",
      "Epoch 60 \t Loss=0.2288 \t Model: y = 0.3576x1 + 0.1436x2 + -4.47e-10\n",
      "Epoch 70 \t Loss=0.2131 \t Model: y = 0.3965x1 + 0.1519x2 + -8.103e-10\n",
      "Epoch 80 \t Loss=0.2007 \t Model: y = 0.4313x1 + 0.1574x2 + -6.985e-10\n",
      "Epoch 90 \t Loss=0.1908 \t Model: y = 0.4626x1 + 0.1607x2 + -4.936e-10\n",
      "Epoch 100 \t Loss=0.1828 \t Model: y = 0.4909x1 + 0.1621x2 + -6.147e-10\n",
      "Epoch 110 \t Loss=0.1763 \t Model: y = 0.5165x1 + 0.162x2 + -7.87e-10\n",
      "Epoch 120 \t Loss=0.1709 \t Model: y = 0.5397x1 + 0.1606x2 + -5.821e-10\n",
      "Epoch 130 \t Loss=0.1664 \t Model: y = 0.5609x1 + 0.1581x2 + -9.08e-10\n",
      "Epoch 140 \t Loss=0.1625 \t Model: y = 0.5802x1 + 0.1549x2 + -9.965e-10\n",
      "Epoch 150 \t Loss=0.1592 \t Model: y = 0.5979x1 + 0.1509x2 + -9.756e-10\n",
      "Epoch 160 \t Loss=0.1564 \t Model: y = 0.6142x1 + 0.1465x2 + -4.144e-10\n",
      "Epoch 170 \t Loss=0.1539 \t Model: y = 0.6292x1 + 0.1416x2 + -1.001e-10\n",
      "Epoch 180 \t Loss=0.1518 \t Model: y = 0.643x1 + 0.1364x2 + -3.236e-10\n",
      "Epoch 190 \t Loss=0.1498 \t Model: y = 0.6559x1 + 0.131x2 + -6.286e-11\n",
      "Epoch 200 \t Loss=0.1481 \t Model: y = 0.6678x1 + 0.1255x2 + 2.119e-10\n",
      "Epoch 210 \t Loss=0.1466 \t Model: y = 0.6789x1 + 0.1199x2 + -1.956e-10\n",
      "Epoch 220 \t Loss=0.1452 \t Model: y = 0.6892x1 + 0.1142x2 + -1.758e-10\n",
      "Epoch 230 \t Loss=0.1439 \t Model: y = 0.6989x1 + 0.1085x2 + -4.307e-11\n",
      "Epoch 240 \t Loss=0.1428 \t Model: y = 0.708x1 + 0.1029x2 + 3.376e-10\n",
      "Epoch 250 \t Loss=0.1418 \t Model: y = 0.7165x1 + 0.09736x2 + 2.841e-10\n",
      "Epoch 260 \t Loss=0.1408 \t Model: y = 0.7245x1 + 0.09189x2 + 3.295e-10\n",
      "Epoch 270 \t Loss=0.14 \t Model: y = 0.732x1 + 0.08653x2 + -8.033e-11\n",
      "Epoch 280 \t Loss=0.1392 \t Model: y = 0.7391x1 + 0.08128x2 + 1.141e-10\n",
      "Epoch 290 \t Loss=0.1385 \t Model: y = 0.7458x1 + 0.07616x2 + 1.321e-10\n",
      "Epoch 300 \t Loss=0.1378 \t Model: y = 0.7522x1 + 0.07118x2 + 5.087e-10\n",
      "Epoch 310 \t Loss=0.1372 \t Model: y = 0.7582x1 + 0.06634x2 + 7.398e-10\n",
      "Epoch 320 \t Loss=0.1367 \t Model: y = 0.7639x1 + 0.06165x2 + 6.845e-10\n",
      "Epoch 330 \t Loss=0.1362 \t Model: y = 0.7693x1 + 0.0571x2 + 8.423e-10\n",
      "Epoch 340 \t Loss=0.1357 \t Model: y = 0.7744x1 + 0.0527x2 + 9.252e-10\n",
      "Epoch 350 \t Loss=0.1353 \t Model: y = 0.7793x1 + 0.04845x2 + 1.104e-09\n",
      "Epoch 360 \t Loss=0.1349 \t Model: y = 0.784x1 + 0.04435x2 + 1.145e-09\n",
      "Epoch 370 \t Loss=0.1346 \t Model: y = 0.7884x1 + 0.0404x2 + 1.631e-09\n",
      "Epoch 380 \t Loss=0.1343 \t Model: y = 0.7926x1 + 0.03658x2 + 1.446e-09\n",
      "Epoch 390 \t Loss=0.134 \t Model: y = 0.7966x1 + 0.03291x2 + 1.429e-09\n",
      "Epoch 400 \t Loss=0.1337 \t Model: y = 0.8004x1 + 0.02938x2 + 1.694e-09\n",
      "Epoch 410 \t Loss=0.1334 \t Model: y = 0.8041x1 + 0.02598x2 + 1.697e-09\n",
      "Epoch 420 \t Loss=0.1332 \t Model: y = 0.8076x1 + 0.02271x2 + 2.125e-09\n",
      "Epoch 430 \t Loss=0.133 \t Model: y = 0.8109x1 + 0.01957x2 + 2.292e-09\n",
      "Epoch 440 \t Loss=0.1328 \t Model: y = 0.8141x1 + 0.01655x2 + 2.913e-09\n",
      "Epoch 450 \t Loss=0.1326 \t Model: y = 0.8171x1 + 0.01366x2 + 3.412e-09\n",
      "Epoch 460 \t Loss=0.1325 \t Model: y = 0.82x1 + 0.01087x2 + 3.749e-09\n",
      "Epoch 470 \t Loss=0.1323 \t Model: y = 0.8228x1 + 0.008204x2 + 3.499e-09\n",
      "Epoch 480 \t Loss=0.1322 \t Model: y = 0.8254x1 + 0.005641x2 + 3.663e-09\n",
      "Epoch 490 \t Loss=0.1321 \t Model: y = 0.828x1 + 0.003183x2 + 4.2e-09\n",
      "Epoch 500 \t Loss=0.132 \t Model: y = 0.8304x1 + 0.0008239x2 + 4.138e-09\n"
     ]
    }
   ],
   "source": [
    "#如果要对我们的模型参数进行训练我们需要创建一个会话。\n",
    "with tf.Session() as sess:\n",
    "    # 初始化全局变量\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 开始训练模型\n",
    "    # 因为训练集较小，所以每轮都使用全量数据训练\n",
    "    #表示我们训练epoch轮我们的数据，一轮代表一次数据流图的训练，也就是我们一次epoch就是训练我们的47组数据，当我们进行一次全量数据集的训练，我们就可以理解为我们进行了一次epoch。\n",
    "    for e in range(1, epoch + 1):\n",
    "        sess.run(train_op, feed_dict={X: X_data, y: y_data})\n",
    "        if e % 10 == 0:\n",
    "            loss, w = sess.run([loss_op, W], feed_dict={X: X_data, y: y_data})\n",
    "            log_str = \"Epoch %d \\t Loss=%.4g \\t Model: y = %.4gx1 + %.4gx2 + %.4g\"\n",
    "            print(log_str % (e, loss, w[1], w[2], w[0]))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
