# TensorFlow张量和变量：


## TensorFlow张量：
__张量：__
	
在数学里张量是一种几何实体，广义上表示任何形式的“数据”，张量可以理解为0阶(rank),标量、一阶向量和2阶矩阵在高维空间上的推广，张量的阶描述它表示数据的最大维度。

__举个例子：__

张量可以理解为在我们0阶的时候我们称它为标量比如说1，2，3，4，5，6，7，8，在一阶的时候我们可以称他为向量，比如中学的平行四边形原理，其实就是向量相加。矩阵就是我们二阶的张量，就像下面我们举得例子这样的两个维度的数据了。除此以外我们还可以往高维空间上做推广，比如说三阶的张量，其实就像数据立方一样，三阶以上我们就很难在平面图中展现出来了，大家可以在脑海中想象，或者说能抽象的去定义他，然后在之后直接去使用这个数据就好了，而不需要想象出他是一个什么样的形状。



__三阶的图形：__
	
他不仅在平面这一层是有数据的，其实张量就是这样一种东西，在数学中表示任意的一种数据，在我们的模型训练中当然可能会遇到各种各样的数据，所以我们在TensorFlow中里面引入了张量的定义，------维基百科



__在TensorFlow中张量的实现方式：__
	在TensorFlow中张量表示某种相同"数据类型"的"多维数组"
		张量的两个重要属性：
			
		1.数据类型(如：浮点型，整型，字符串)
		2.数组形状(各个维度的大小)

__其实这里说的数组就是张量的形状的一个定义：__
	
	比如刚开始的时候我们有提到一个一阶，二阶，三阶的数组，但是比如说我们现在定义一个三阶的数组，但是这个三阶数组的个个维度是长什么样的呢？就比如说一个数组是三阶，那第一阶的维度是多少呢？ 假设我们将平面这一层定义为第一阶第二阶，我们可以看出他第一阶和第二阶的维度就是3。

深度这一阶我们根据图可以看出是2，这个其实是一个很重要的概念，大家可以将张量的阶理解成这个张量有多少个维度。

	比如说他说一个三阶的张量，那么他就有三个方向可以去延展，但是每一个维度里面，或者说每一阶里面，他的维度值有多大，他有多长，他的尺寸是多少，这些都是需要明确定义的，比如说你现在只告诉我你想要做一个数据立方，如果没有告诉我维度，那这个数据立方可以是各种各样的比如说一个3*3*2这样一个形状的数据立方，也有可能是一个3*3*4的数据立方，所以这里需要一个形状这样的一个属性来描述我们的张量。

	但是这里要注意的是必须是某种相同类型的多维数组，因为当你一个张量中描述的数据类型不一样的时候其实是很难去进行计算的，大家可以去想象一下一个二阶的张量，当我们这个二阶数组的第一个元素是整型，第二个元素是浮点型，第三元素是字符串的时候，如果想要取和其他的数组去做计算的时候其实是很难进行计算的。所以在模型训练中也不太会有这样的场景出现。所以说TensorFlow也没有基于这样的实现。



所以我们在定义张量的时候我们要注意，要保证张量中的元素数据类型是一致的，并且明确的描述出这个数组的形状。


__在TensorFlow中张量是什么？__

1.张量是用来表示多维数据的。 
	 
	 我们可以用它来表示一维二维三维n维的数据，其实n阶的一个数组是可以被一个多维的数组表示出来。

2.张量是执行操作时的输入或输出数据。

	在之前我们讲过的数据流图中，边里面其实流动的就是数据，那这个边就是我们的张量，每一个节点就是我们的操作，在我们执行操作的时候是需要输入数据的，之前我们就有举过一个y=a+b的一个例子，a和b其实就是我们的输入数据，这个是要通过张量将我们的数据流到操作节点上进行运算，当我们算完a+b的结果等于y，那么这个y就会作为下一个操作的输入数据。所以说a+b这个操作输出的数据也是用张量来表示的，当我们去执行操作的时候就需要张量这么一个抽象来表示数据，并且输入到我们的操作节点当中。

3.用户通过执行操作来创建或计算张量
	
	那我们如何创建一个张量呢张量的创建其实不需要通过类的构造方法去创建的，而是可以通过执行一个操作来创建的

4.张量的形状不一定在编译时确定，可以在运行时通过形状推断计算得出。

	开头的时候我们说过形状的定义很重要，我们这里说的形状定义是指在实际的运行时，必须要把形状定义下来，比如说我们现在做一个两个矩阵的相乘，但是你现在不清楚矩阵的形状，你是无法去做相乘的。但是我们在写代码的时候是不一定知道形状的，因为这是和你的输入数据相关的，我们刚开始的时候有看到在我们的数据流图里面，input这个数据节点是我们的输入数据，输入数据如果大家有训练模型的基础的话，可能会知道一些超参数，比如我们每一个批数据(batch_size)到底是多少，这个就决定了当我们进行大量的矩阵运算的时候，初始的形状到底是什么，所以说通俗的做法是，我们在定义一个模型参数的时候，我们可能会把我们的形状某一个部分是预留的，也就是说某一个部分不是马上确定的，我们可以通过一个缺省的方式先留在那，然后在实际运行的时候通过形状推断，比如说我们的batch_size设置为32，最后能推断出运行到这一步的时候，运算到这一层的时候，当前的这个张量，当前的这个数组到底是什么形状。所以说这个也是TensorFlow在实现计算的时候为大家预留的一个很好的特性使得我们的计算可以更加的灵活，如果没有这样的一个特性，可能 每当我们去画一个batch_size的时候都要去改一遍代码，这个是非常的不灵活的。


__TensorFlow中几类特殊的张量，由一下操作产生：__


	tf.constant //常量

	tf.placehodler //占位符

	tf.Variable //变量


刚刚有提到张量是由操作产生的，通过执行一些操作我们可以去操作和计算一些张量，操作其实有很多种，这里我们举了几类特别的张量
			
__常量：__
	
	比如说常量的操作，常量是很多语言中都有的一个概念，常量操作其实就是一种最普通的张量当使用常量操作创造出张量这个张量的值是不可改变的，这个不可改变其实是张量中一个很重要的概念，也很容易和传统的编程语言搞混，就因为在一个特定的操作执行过程当中，张量一定是不可改变的，大家可以想象在我们的数据流图中流动的时候，从一个节点流动到另一个节点，这条边中的数据这个值是不可改变的，也就是说当我们执行一个y=a+b的时候这个a和b的值是无法改变的，当你执行完一个a+b的时候，你可能会有新的输入数据，会再次执行a+b这个时候可能因为你的输入数据变了a和b就已经变化了，但是这个时候你的值就无法用常量的操作去创建，因为常量操作创建出来的值，是不论你怎么改变输入数据他都是不能改变的一个量，大家可以理解为这是一个TensorFlow编程中的一个常		

__placehodler：__
			
	可以简单的理解为他说描述数据的一个壳，本身是需要通过数据流图外面的数据填充进来，才可以得到数据的一种操作，如何填充这样的数据我们之后会做介绍
				
	例子：比如我们定义一个三阶的张量，我们可以想象我们的形状是 3* 3 *2 ，如果我们使用常量来定义的话，这个数据形状以后就无法改变除非你去定义一个新的操作，重新得到一个新的值，但是如果你是使用占位符的话，其实你是得到一个3*3*2这个数据立方的壳，里面是没有任何的值的，当你每次从数据流图外向里面填充数据的时候，在那个时刻，才能得到数据，比如说我们现在的模型里面的值是123456789，之后如果想要把他覆盖再填充也是没有问题的，而且你可以不用一次性的完整定义形状 而只是先定义一个3*3然后空下来一个维度，然后这个立方就变成了一个平面确定，深度不确定的一个立方，然后在外面填充数据的时候再去定义这个深度有深。这个就给模型的训练和数据的填充留下了一个灵活的接口。

__变量：__
	
	维护特殊张量的状态。使得这个值在数据流图运行结束后也不会被释放，会一直常驻内存使得我们的值可以保存下来。


__总结：__
  
	TensorFlow的张量其实是表示数据的，可以表示任意维度的数据，在张量中有一个特殊的张量也就是常量，用常量创建出的张量是不可以改变的，占位符产生的其实就是一个高维数据的壳，他需要在图外填充数据，在运行时的时候才可以得到一个确定的张量，还有一个变量他可以用来维护特殊张量的状态。使得这个值在数据流图运行结束后也不会被释放，会一直常驻内存使得我们的值可以保存下来。





__TensorFlow张量(下)：__

	

	
tf.Variable(参数，数据类型(可以不写，不写的时候由TensorFlow自己推断)，变量名(可以默认不写))
以下是Variable输出的内容的含义

	分别是在内存中的名字     他们的形状      数据类型
	variable：0                shape          dtype=int..
	variable_1：0              shape          dtype=..
	variable_2：0              shape          dtype=..
	variable_3：0              shape          dtype=..



阶和维度：
	
	其实阶就是定义了我们一个张量描述的最大维度有多少个，比如说我们是一个三阶的张量我们可以在x,y,z直角坐标系图中任意的去表示，如果我们是一个二阶的张量，我们是无法突破第三个维度的

![Image_text](https://raw.githubusercontent.com/OneStepAndTwoSteps/TensorFlow_notes/master/static/%E6%95%B0%E6%8D%AE%E6%B5%81%E5%9B%BE2.jpg)

## TensorFlow变量：

TensorFlow变量的作用主要作用是维护特定节点的状态，如深度学习或者机器学习的模型参数

__什么是模型参数：__
	
	之前在数据流图中有给大家介绍过权重和偏置 w 和 b，这两个其实就是我们的模型参数。

我们将我们的参数在模型中进行计算直到updata更新之后，变量也不会在内存中释放，而是常驻在内存里面，下一次我们还要将 w 和 b 带入到模型中进行优化更新。

	tf.Variable方法本身其实是一种操作，返回的值就是变量(特殊张量) 特殊就特殊在，当我们完成一次更新操作的时候，变量里面的数据不会被释放，而是常驻内存

__变量和张量的异同：__

同：

	通过tf.Variable方法创建的变量，与张量一样，可以作为操作的输入输出

异：

	张量的生命周期通常伴随着依赖的计算完成而结束，内存也随之释放 (当没有被依赖的时候，表示这个张量不会再被使用，就可以进行垃圾回收了)

变量则常驻内存，在每一步训练(一步训练可以理解为完整的走一次数据流图)时不断更新其值，以实现模型参数的更新

深度神经网络其实就可以通过训练更新模型参数的值，整一个数据流图可以大致的分为，前向计算，比如说从input到我们预测出值然后通过交叉熵(Cross Entropy)还算出损失值，然后进行梯度，通过梯度下降更新我们的参数值，进行梯度及其后面部分我们把它称为后向传播，通过前向传播和后向传播我们完成一次完整的训练。这个时候我们可以看出我们这次训练已经训练完成，如果变量像普通张量一样没有依赖去把内存释放掉，我们的模型参数也就是无法显式的存储在我们的图里面，在下一次训练的时候，我们有需要再去初始化我们的模型参数，这样每一步训练我们都要去重载模型参数，这样的开销是不必要的，所以我们的变量会常驻内存。

	import tensorflow as tf
	
	#创建变量
	w= tf.Variable(<initial-value>,name=<optional-name>)   TensorFlow可以根据你输入的值判断你的变量类型，或者说你通过之前的计算推出你的变量类型
	
	#将变量作为操作的输出
	y=tf.matmul(w,...another variable or tensor...)        w可以作为一个参数，和其他的变量或者张量进行计算，然后运算得出y
	z=tf.sigmoid(w+y)                                      操作出的值还可以在进行计算，所以说变量在计算方面和张量并无区别，主要区别就是变量常驻内存

	#使用assign 或 assign_xxx 方法重新给变量赋值           可以用来初始化或者给变量赋值
	w.assign(w+1.0) 
	w.assign_add(1.0)

__注意：__
	
	在开始会话之前，也就是我们在开始图运算之前，我们是没有任何计算运行的，而只是一个抽象的图的定义没有执行，如果我们想要执行的话，我们需要加载数据流图到我们的会话里面，然后通过初始化我们的变量才能开始运算，所以我们在开始执行我们的数据流图的时候，要初始化我们所有变量。

定义和初始化是两个完全分开的步骤：
			
	定义是描述我们的函数长什么样子，他的初始值是什么。

	但是如果我们要计算的时候要在会话里面进行计算，你需要去申请对应的资源，包括比如你如果想在cpu上计算，你需要把你的函数和数据流图放在对应的设备上面，比如说放在cpu或者GPU，当你将它放到设备上要进行计算的时候就需要执行一个初始化的一个操作。这个操作等等我们会做展示。

	TensorFlow提供了一个给全局所有变量初始化的一个操作叫做 tf.global_variables_initializer() 它会把我们之前定义值全部填充到模型中去，如果我们知道他的内部执行原理的话，我们可以知道其实TensorFlow在其内部执行的是assign方法。


![Image_text](https://raw.githubusercontent.com/OneStepAndTwoSteps/TensorFlow_notes/master/static/Tensor_and_Variable/%E4%BD%BF%E7%94%A8%E6%B5%81%E7%A8%8B.jpg)

## TensorFlow变量的使用流程：

__tf.Variable 管理的三个模块__
	
三个模块

	创建 创建变量

	初始化：将数据流图加载到会话当中，然后通过全局初始化的操作，真正的把值赋给我们变量。

	更新：更新变量实际上操作的就是assign函数               更新后会将参数--->存储到checkpoint文件中


tf.train.Saver:

__tf.train.Saver类：__
	
	是一个和真正的模型训练相关的类 当我们训练一个很大的模型时 我们不能一下就训练成功，我们可能需要在训练中途的时候停下来修改一些东西，比如说调整一些超参数，这个时候我们需要将我们的模型参数持久化，保存到一个文件里面，这个时候我们就需要通过Saver把需要训练的模型参数对应的变量保存下来，保存下来的东西我们就把它称为checkpoint文件，这个时候我们就可以把我们的参数保存到checkpoint文件，保存下来之后，我们可以再进行参数的恢复，
	
两个模块

	恢复 恢复之前保存在checkpoint中的参数

	存储 存储恢复之后的参数，存储更新后的参数

tf.random_normal 正态分布初始化
