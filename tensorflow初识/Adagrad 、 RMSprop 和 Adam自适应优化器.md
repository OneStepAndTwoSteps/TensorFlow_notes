### 优化器介绍：Adagrad – RMSprop – Adam 

看Adagrad，相比于Momentum(动量)(SGD-M在随机梯度下降中加入了动量的概念)引入了二阶动量的概念，其实SGD也好Momentum也好他们都说以相同的学习率更新我们的模型的各个分量，在深度学习里面其实他的参数量非常大有的参数更新的非常频繁有的参数更新的就不是非常频繁，比如  word2vec词向量更新的时候，有一些低频词甚少更新模型参数，高频的词汇肯就是在不断的学习，对于这样的情况，我们就希望对于，更新不频繁的参数，我们希望他的单次的步长更大一些多学习一些知识，更新频繁的参数我们希望它更新的频率更小一些，使得他学习到的参数更稳定，而不至于被单个样本影响太多，Adagrad算法就可以达到这样的效果。但是Adagrad算法的学习率是一个逐渐递减的过程，可能会导致我们的训练过程提前结束，这个时候RMSprop就改进了这一缺点。


RMSprop在计算二阶动量vt的时候，只关注最近某一个时间窗口内的下降梯度(我们可以看到在Adagrad算法中我们是累计所有的二阶动量，但是RMSprop只关注了最近一个窗口的二阶动量) 采用指数移动平均的方法求我们的二阶动量

Adam结合了Adagrad和RMSprop的优点，不仅对二阶动量(vt)使用我们的指数移动平均，同时对我们的一阶动量(mt)也是有了指数移动平均，这样可以使我们的训练过程更加平稳
