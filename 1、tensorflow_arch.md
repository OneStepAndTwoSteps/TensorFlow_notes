# tensorflow 的架构和数据流图

  ![Image text](https://raw.githubusercontent.com/OneStepAndTwoSteps/TensorFlow_notes/master/static/module_APIs.jpg)

## TensorFlow模块和API：

高层次的 API 有大家熟悉的 Estimators 和 Keras 
		
	Estimators (是TensorFlow团队提供的模型试用角度的一套高层次的API，主要提供了模型估算，模型定义等模型指标的接口)
	Keras (早期的一个机器学习框架，自从TensorFlow开源以来很好的对接了TensorFlow，现在也成为了TensorFlow一个高层次的API)

中层次的 API 提供了像 Layers  Datasets   Metrics

	Layers也就是像我们在神经网络中常用到的 像一些 卷积神经网络和石化神经网络等等这些层，其实都可以被封装好了 成为了其对应的接口

	我们数据的输入，输出也有一个很好的处理模块Datasets

	Metrics就是我们训练过程中的各种指标

低层次的API：像 Python C++ Java Go

	这些语言都是在应用层对TensorFlow进行的封装，真正的核心层在TensorFlow Kernel

TensorFlow Kernel	

	分布式执行引擎，和本地执行引擎


因为中层次和高层次的TensorFlow API现在还在迭代，这里就不深入讲解，接下来讲一下 TensorFlow Kerenl和低层次的API

  ![Image text](https://raw.githubusercontent.com/OneStepAndTwoSteps/TensorFlow_notes/master/static/arch.jpg)

## TensorFlow的架构：
	
TensorFlow的架构遵循了分层次和模块化的设计思想，每一个高层次的模块都会去调用低层次的模块，低层次的模块也会开放接口，向上提供调用的逻辑，像Python和C++这种低层次的API就是为TensorFlow运行时核心提供多语言的支持，我们可以看到像一些Python API Java API 其实都是基于最底层的C API的封装，C++ API其实是自己完整的实现了应用层的封装


TensorFlow运行时核心：(主要就是实现了我们TensorFlow中的一些关键性的数据结构，比如Tensor(张量)的定义和一些operation(操作)的定义)
	
	公共运行时：提供了数据流图的运行时的逻辑和核心
			
		这里有一些关于TensorFlow数据流图的一些概念。比如图执行器和图优化器。之后会介绍

		会话：提供了一个完整的生命周期，给我们图的执行提供了运行环境 之后会介绍
			
		会合点：为了保证进程间的通信，提供了会合点的机制，
			
		内存管理器：管理内存

	核函数：
			
		TensorFlow的操作实现都是由核函数实现的，比如在CPU上如何进行加法运算，我们平时调用的Python的API其实都是一个封装，最后都是调用核函数来实现


	XLA：设计理念：因为TensorFlow的细粒度化，所以TensorFlow很灵活，但在我们运行的时候为了保证运行的效率足够高，XLA通过一些中间表示层和编译器这些优化技术将我们一些细颗粒度的操作结合成效率更高的粗粒度的操作这样可以使我们的运行效率更加的高，同时也可以做到异构架构之间的解耦

	通信网络：

	计算设备：



	TensorFlow对大规模的计算有很好的支持
	分布式运行时：将数据流图放在不同的设备上去运行，在不同的进程间运行，之间通过gRPC通信，结合远程会合点的设计理念，同时设计了一套很好的调度器

		gRPC：提供不同设备或进程的通信

		采用master + worker的主从设计结构



综述：
	
	整个从底层的通信网络，到计算设备，计算网络层的设计到之上的我们提供的核函数和XLA，再然后将这些接口封装成数据流图的抽象以及图的相关概念，在往上实现TensorFlow的分布式运行，把我们的主从设计结构，和大规模的通信都能进行很好的调度，将这整个运行时核心设计完成之后，TensorFlow再在上面封装了很多语言的接口，再在往上的中层次和高层次的API。




## tensorflow数据流图：



  ![Image text](https://raw.githubusercontent.com/OneStepAndTwoSteps/TensorFlow_notes/master/static/编程范式.jpg)


编程范式：

TensorFlow的数据流图其实是一种声明式的流程范式；

编程语言里面有很多不同的编程范式，比较典型的两种编程范式是“声明式编程范式”和“命令式编程范式”。

	对于大多数编程中我们偏向的一般都是命令式的编程范式。他一般都像是告诉计算机该怎么做事情，坑比更偏向于计算过程中的状态转换，如果了解有穷自动机的话，应该就会了解有穷自动机就是在不同的状态之间转换，通过一些方法，从一个状态转换成另一个状态，其实命令式编程大多数时候都是用于 一些业务逻辑的实现，比如在我们设计一个游戏的时候，角色做了什么操作会有什么反应，或者说他从这一级升到下一级会有一个什么状态切换，他给计算机输入的也更想是指令一样的东西，本身的实现方式可能导致它更加的过程化和具体化。


	但TensorFlow的数据流图是一种声明式编程，更多的是聚焦于我要什么，之前我们在实现一个函数的时候，更多的是在想要在函数里面实现一个什么东西，比如：他要去怎么去实现两个数字的相加或者矩阵相乘，用声明式编程范式更多的是聚焦于结果是什么，里面的具体实现先不去想，所以他是偏向数据模型抽象算法的一种设计其本身也就是一些非状态转换而是表达式变换的一种设计，大家如果写过lambda函数应该是有这种体会的，你会是更加聚焦于x和y，中间是什么其实是比较明朗的，他的计算单元也不在是指令而是函数，而不是说不同变量之间如何去操作，然后使得加出一个什么样的数，而更多的是我们像数学函数一样定义 y=f(x) ,这个f(x)要怎么去变换，这样的一种逻辑思想，所以他更擅长于一种数理逻辑的一些领域，比如说我们神经网络的定义，就是比如我们要去想象我们有很多的神经网络层，一开始可能是一个全连接层，接下来可能是一个卷积层，接着是磁化，接着再是激活，如果我们使用命令式的编程方式，你可能会想到的是全连接从里面该会有多少的矩阵相乘等等，这样是不便于我们进一步抽象的所以说这种声明式的编程方式会更加的使用于一些结构化抽象话的一种方法。

斐波那契数列示例：

声明式编程我们就可以很容易的看出这个lambda函数表达的是实现一个斐波那契数列的

声明式编程：

	fib=lambda x: 1 if x <= 2 else fib(x-1)+fib(x-2)

当前这个值就是 前两个数字之和 里面定义的条件是当x<=2的时候我们现在所在的位置是这个数列中的第一个数和第二个数，那么就将结果都默认 设置为1
从第三个数字开始就要遵循 这个数 = 前面两个数的数字之和这样的规则。

所以lambda函数是更适合于函数的表达模式


但有的人可能会有疑惑？这里使用递归的方法效率会不会比较低？
	
	其实像TensorFlow这样专门为声明式编程去做了设计的库，其实他在底层会有大量的优化 就比如编译器的一些优化的技术，可以做一些预编译，可以把很多的一些可以合并的操作在编译的时候就进行优化，而不会再运行的时候再不断的去迭代去递归，所以大家不用担心效率的问题，后面我们也会为大家进行一个对比展示。 




如果我们一眼看去命令式编程的斐波那契函数，我们是无法直接看出他到底是做什么的。熟悉斐波那契数列的人也只能将代码看完，才能知道这个函数表达的含义

命令式编程：

	def fib(n):
		a,b=1,1
		for i in range(1,n):
			a,b=b,a+b

		return a


  ![Image text](https://raw.githubusercontent.com/OneStepAndTwoSteps/TensorFlow_notes/master/static/数据流图1.jpg)
  ![Image text](https://raw.githubusercontent.com/OneStepAndTwoSteps/TensorFlow_notes/master/static/数据流图2.jpg)

## TensorFlow数据流图:

数据流图作为图，图中有两个很重要的元素 边和节点。

	边：
		在TensorFlow中的边都是有向边(都是有方向的)，明确的指出了数据的流向是从哪一边指向哪一边，从哪一个节点流向了另一个节点。

	节点：
		节点就是说我们实际上进行的操作了。

首先从边开始讲：


	边首先分为张量和稀疏张量，张量在TensorFlow中我们可以先简单的理解为数据，通过图我们可以看到我们的数据都是从一个节点流向了另一个节点，比如input就是我们输入的一个数据，我们的训练数据其实就是从这里输入，输入到图里面我们可以看到经过了两个layer，一个是ReLu Layer (激活层) 一个是Logit Layer全连接层算出的预测值，那其实我们的数据就是不断的在图里面流动。

张量(Tensor)和稀疏张量(SparseTensor)的区别：

	其实我们在神经网络中使用的数据集都是非常高纬度的数据集，张量其实就是一种能比较好的表示出高维数据的一种抽象，其使用了一种多维数据的一种表达方式去描述这个高维数据，但是很多时候我们的维度很高，比如一千维一万维，甚至十万维的数据，但是它里面的数据可能是想当稀疏的，他里面的很多值可能都是0或者说对我们的计算结果没有很多影响这个时候如果我们要把它全部存下来是非常浪费的，那这就类似于我们在存一个图的时候会去选择零阶矩阵和零阶表类似的思想，这里的稀疏张量其实也就是这样的意思，它就是想数据集中有意义的内容存下来，其他的就不需要存，然后再将高维数据的形状存下来，然后将对应像索引或者说下标存下来，然后把值存下来通过ket:value的形式就能把稀疏张量表示出来，而不需要一个完整的数组，现在我们使用张量和稀疏张量来表示出高维数据，可以用于我们的模型训练，然后通过数据流图中的有向边，来描述出数据流图中的数据是从哪一个节点流向了那一个节点。

节点也有很多不同的类型：
	
这里先做一个简单的抽象，我们通过模型训练的视角可以将节点分为三类，分别是计算节点 存储节点和数据节点

计算节点(operation)：

		用于计算的一些操作包括一些逻辑的操作，神经网络的操作，一些简单的加法的操作和一些规约操作等等 (我们后面会介绍)

假设模型为: s=wx+b
存储节点(variable)：

	其实就是我们的variable就是我们的变量在图中我们可以看出我们的 w (权重) 和 b (偏置) 模型的权重和偏置,那模型的权重和偏置就是我们数据流图或者说是神经网络的训练的对象，我们其实上就是为了实现一个能解决我们实际上问题的w和b，也就是像找到一个能解决出我们问题的函数，w和b就非常重要，其实我们的逻辑运算，我们的神经网络层，我们的一些矩阵相乘这些逻辑其实都是不变的，但是我们输入的数据会变化，我们每一次训练都会丢入新的数据来训练，那这个时候，我们的w和b就是会不断的迭代更新的，我们的variable其实就是用来存放我们迭代的模型参数的

数据节点(placeholder)：

	也就是我们图里面的input，input其实包含class labels(分类模型里面的标签) 在监督学习里面我们通常会使用这种方式。

	这时候我们其实是需要输入新的数据到数据集里面的，他们不是这个图里面的计算逻辑，而是说我们的训练数据，我们的测试数据，其实这些数据也是需要我们去描述和表达的，Placeholder其实就是用来表示图外输入的数据，它回去描述数据的类型，数据的形状，当这个图描述完成之后，还没有真正开始进行运算，真正开始运算其实是从input节点开始的，当我们真正把数据输入到数据流图之后才会真正的开始训练，当图走完之后，我们通过损失值算出了梯度，通过梯度下降我们求出了我们优化的值是多少然后将我们优化出的值更新我们的这一轮的模型参数，然后一轮的模型也就更新完成。

入度和出度：
		
	如果我们关注图中的入度和出度，我们可以发现图中入度为0的点是input点，其他的点都是入度大于等于1的点，对于一个图的执行逻辑来说
	如果大家做过拓扑排序这样的算法，就会知道input 是一个非常关键的点，从他可以开始整个图的运算，整个TensorFlow数据流图其实也就是遵循了这样的原理，为了保证TensorFlow的数据流图不出错，其实就是从入度为0的点开始算，这张图中入度为0的点是input，但是input需要数据才能开始计算，所以说我们需要从图外导入数据，当我们从图外导入数据的时候，这个图就被点亮了，点亮之后，他会将数据传到下一个节点，当传到下一个节点的时候，下一个节点就会变成入度为0的点，所以这个节点也被放入到了执行队列里面去运算，当reshape操作执行完成之后，它所连接的矩阵相乘操作也就开源开始计算，接着，我们整个数据流图就会按照图中的顺序运算，直到最后完成训练


TensorFlow的执行原理：

	其实TensorFlow的执行原理就是，通过一个可执行队列将入度为0的节点，放入到这个队列当中，每执行完一个节点，他就会更新它所连接的哪些节点，然后去更新他们的入度，当有新的节点入度为0的时候，就会被放到这个队列当中，当然这些入度为0的点其实本身也不会互相冲突，所以可以进行并行计算，这个也会导致我们的数据流图效率很高。


数据流图的优势：
				
快：

	可以参考TensorFlow自己公布的测试数据来看出一些性能对比。

	分别是采用一个两个四个和8个Gpu的时候相同的神经网路模型中的性能对比。

	纵轴是每秒处理的图片数量，
	当GPU数量为1时每秒钟处理30张图片，当GPU数量为2时每秒钟处理58张图片…………，可以看出是接近线性的增长的水平

				

为什么可以得出这个结论呢？	

	因为：
	GPU分别为1 2 4 8 并行计算没有对TensorFlow性能造成多大影响，其处理图片的数量呈接近线性的增长的水平。
	结论：
	可以看出TensorFlow模型在并行处理的时候非常的快，在并行计算的时候不会大量开销资源而导致运行速度变慢




并行计算快：

	因为TensorFlow的数据流图采用可执行队列和拓扑排序的思想，使得我们数据流图在执行的时候明确了先后的依赖顺序，实际上TensorFlow中的图如果使用tensorboard可视化展现出来的时候，图中的边就要很好的控制依赖的作用，这个也和我们拓扑排序思想是一样的，当有边输入到节点的时候你的入度是大于0的，所以你是不能执行的，当我们有数据依赖，也就是我们需要一个数据才能执行，比如说y=a+b，所以我们需要知道a和b这两个数据，否则无法计算，所以通过这种方式我们可以更好的做好控制依赖，除了数据作为控制依赖，TensorFlow也支持显示的去设置控制依赖，这也有利于我们并行的优化



分布式计算快：

	在CPU,GPU,TPU上都可以计算的很快


预编译优化XLA：

	可以把细粒度的操作结合成大颗粒的操作，提高效率


可移植性好：

	多语言方面的支持，使得TensorFlow编译出的代码和语言无关，特别是加入XLA之后。甚至和很多架构是无关的，

IO性能：

	随着GPU的增加，IO的加速比其实并没有增加多少






